{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVpko9I7ymuMGjxWsvxYB4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-KomgkwWPBln"},"outputs":[],"source":["import streamlit as st\n","from transformers import BartForConditionalGeneration, BartTokenizer, AutoTokenizer, AutoModel, AutoModelForQuestionAnswering\n","import torch\n","from docx import Document\n","import pdfplumber\n","from io import StringIO\n","import os\n","from googletrans import Translator\n","\n","# Load models\n","@st.cache_resource\n","def load_bert_model():\n","    class BERT_Arch(torch.nn.Module):\n","        def __init__(self, bert):\n","            super(BERT_Arch, self).__init__()\n","            self.bert = bert\n","            self.dropout = torch.nn.Dropout(0.1)\n","            self.relu = torch.nn.ReLU()\n","            self.fc1 = torch.nn.Linear(768, 512)\n","            self.fc2 = torch.nn.Linear(512, 2)\n","            self.softmax = torch.nn.LogSoftmax(dim=1)\n","\n","        def forward(self, sent_id, mask):\n","            cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n","            x = self.fc1(cls_hs)\n","            x = self.relu(x)\n","            x = self.dropout(x)\n","            x = self.fc2(x)\n","            x = self.softmax(x)\n","            return x\n","\n","    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","    bert = AutoModel.from_pretrained('bert-base-uncased')\n","    model = BERT_Arch(bert)\n","    model.load_state_dict(torch.load(\"fakenews_bert_weights.pt\", map_location=torch.device('cpu')))\n","    model.eval()\n","    return tokenizer, model\n","\n","@st.cache_resource\n","def load_bart_model():\n","    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","    return tokenizer, model\n","\n","@st.cache_resource\n","def load_qa_model():\n","    qa_model_name = \"distilbert-base-cased-distilled-squad\"\n","    qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n","    qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n","    return qa_tokenizer, qa_model\n","\n","# Utility functions\n","def extract_text(file):\n","    file_extension = os.path.splitext(file.name)[1].lower()\n","    if file_extension == \".pdf\":\n","        text = \"\"\n","        with pdfplumber.open(file) as pdf:\n","            for page in pdf.pages:\n","                text += page.extract_text() + \"\\n\"\n","    elif file_extension == \".docx\":\n","        doc = Document(file)\n","        text = \"\\n\".join([para.text for para in doc.paragraphs])\n","    elif file_extension == \".txt\":\n","        text = StringIO(file.read().decode('utf-8')).read()\n","    else:\n","        raise ValueError(\"Unsupported file type.\")\n","    return text\n","\n","def split_text_into_chunks(text, tokenizer, max_tokens=900, overlap=100):\n","    tokens = tokenizer.tokenize(text)\n","    chunks = []\n","    for i in range(0, len(tokens), max_tokens - overlap):\n","        chunk = tokens[i:i + max_tokens]\n","        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n","        chunks.append(chunk_text)\n","    return chunks\n","\n","def summarize_chunk(text, model, tokenizer, max_length=300):\n","    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n","    summary_ids = model.generate(inputs, max_length=max_length, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n","    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","def recursive_summarize(text, model, tokenizer):\n","    chunks = split_text_into_chunks(text, tokenizer)\n","    summaries = [summarize_chunk(chunk, model, tokenizer) for chunk in chunks]\n","    combined = \" \".join(summaries)\n","    if len(tokenizer.tokenize(combined)) > 1024:\n","        return recursive_summarize(combined, model, tokenizer)\n","    return combined\n","\n","def classify_fake_news(text, tokenizer, model):\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","    with torch.no_grad():\n","        outputs = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n","        probs = torch.softmax(outputs, dim=1)\n","        prediction = torch.argmax(probs, dim=1).item()\n","    return \"Fake\" if prediction == 0 else \"Real\"\n","\n","def translate_text(text, target_language=\"fr\"):\n","    translator = Translator()\n","    translated = translator.translate(text, dest=target_language)\n","    return translated.text\n","\n","def answer_question(text, question, model, tokenizer):\n","    inputs = tokenizer(question, text, return_tensors='pt', truncation=True, max_length=512)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        start_idx = torch.argmax(outputs.start_logits)\n","        end_idx = torch.argmax(outputs.end_logits) + 1\n","        answer = tokenizer.convert_tokens_to_string(\n","            tokenizer.convert_ids_to_tokens(inputs.input_ids[0][start_idx:end_idx])\n","        )\n","    return answer if answer.strip() else \"Could not find a relevant answer.\"\n","\n","# ------------------ Streamlit UI ------------------\n","\n","def main():\n","    st.set_page_config(page_title=\"VeriNews AI ‚Äì Detect. Summarize. Translate.\", layout=\"wide\")\n","    st.title(\"VeriNews AI ‚Äì Detect. Summarize. Translate.\")\n","\n","    uploaded_file = st.file_uploader(\"Upload PDF, DOCX or TXT file:\", type=[\"pdf\", \"docx\", \"txt\"])\n","    custom_text = st.text_area(\"Or paste/edit your own news article here:\")\n","\n","    if uploaded_file or custom_text:\n","        text = extract_text(uploaded_file) if uploaded_file else custom_text\n","        if st.button(\"Submit\"):\n","            st.session_state[\"text\"] = text\n","            st.session_state[\"bart_tokenizer\"], st.session_state[\"bart_model\"] = load_bart_model()\n","            st.session_state[\"bert_tokenizer\"], st.session_state[\"bert_model\"] = load_bert_model()\n","            st.session_state[\"qa_tokenizer\"], st.session_state[\"qa_model\"] = load_qa_model()\n","\n","    if \"text\" in st.session_state:\n","        text = st.session_state[\"text\"]\n","        bart_tokenizer = st.session_state[\"bart_tokenizer\"]\n","        bart_model = st.session_state[\"bart_model\"]\n","        bert_tokenizer = st.session_state[\"bert_tokenizer\"]\n","        bert_model = st.session_state[\"bert_model\"]\n","        qa_tokenizer = st.session_state[\"qa_tokenizer\"]\n","        qa_model = st.session_state[\"qa_model\"]\n","\n","        # --- Summarize ---\n","        with st.container():\n","            if st.button(\"Summarize\"):\n","                with st.spinner(\"Summarizing...\"):\n","                    summary = recursive_summarize(text, bart_model, bart_tokenizer)\n","                    st.session_state[\"summary\"] = summary\n","            if \"summary\" in st.session_state:\n","                st.subheader(\"üìÑ Summary\")\n","                st.write(st.session_state[\"summary\"])\n","\n","        # --- Classify News ---\n","        with st.container():\n","            if st.button(\"Classify News\"):\n","                with st.spinner(\"Classifying...\"):\n","                    prediction = classify_fake_news(text, bert_tokenizer, bert_model)\n","                    st.session_state[\"prediction\"] = prediction\n","            if \"prediction\" in st.session_state:\n","                st.subheader(\"üïµÔ∏è Fake News Prediction\")\n","                st.markdown(f\"**Prediction:** {st.session_state['prediction']}\")\n","\n","        # --- Translation ---\n","        target_lang = st.text_input(\"Enter language code (e.g., fr, hi, es):\")\n","\n","        with st.container():\n","            if st.button(\"Translate News\") and target_lang:\n","                with st.spinner(\"Translating full news...\"):\n","                    translated = translate_text(text, target_lang)\n","                    st.session_state[\"translated_news\"] = translated\n","            if \"translated_news\" in st.session_state:\n","                st.subheader(\"üåç Translated Full News\")\n","                st.write(st.session_state[\"translated_news\"])\n","\n","        with st.container():\n","            if st.button(\"Translate Summary\") and target_lang:\n","                if \"summary\" in st.session_state:\n","                    with st.spinner(\"Translating summary...\"):\n","                        translated_summary = translate_text(st.session_state[\"summary\"], target_lang)\n","                        st.session_state[\"translated_summary\"] = translated_summary\n","                else:\n","                    st.warning(\"Please summarize the news before translating it.\")\n","            if \"translated_summary\" in st.session_state:\n","                st.subheader(\"üåç Translated Summary\")\n","                st.write(st.session_state[\"translated_summary\"])\n","\n","        # --- Q&A ---\n","        question = st.text_input(\"Enter your question:\")\n","        with st.container():\n","            if st.button(\"Get Answer\") and question:\n","                with st.spinner(\"Answering...\"):\n","                    answer = answer_question(text, question, qa_model, qa_tokenizer)\n","                    st.session_state[\"qa_answer\"] = answer\n","                    st.session_state[\"qa_question\"] = question\n","            if \"qa_answer\" in st.session_state:\n","                st.subheader(\"‚ùì Q&A\")\n","                st.markdown(f\"**Q:** {st.session_state['qa_question']}\")\n","                st.markdown(f\"**A:** {st.session_state['qa_answer']}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}